{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ FunctionGemma 270M Fine-Tuning\n",
                "\n",
                "Fine-tune **google/functiongemma-270m-it** on your custom function calling data.\n",
                "\n",
                "---\n",
                "\n",
                "## ‚ö†Ô∏è BEFORE YOU START\n",
                "\n",
                "1. **Accept the license:** https://huggingface.co/google/functiongemma-270m-it\n",
                "2. **Get HuggingFace token:** https://huggingface.co/settings/tokens\n",
                "3. **Add token to Colab:** Click üîë key icon ‚Üí Add `HF_TOKEN` ‚Üí Paste your token\n",
                "4. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí T4 GPU"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Check GPU & Authenticate"
            ],
            "metadata": {
                "id": "step1"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Check GPU\n",
                "!nvidia-smi --query-gpu=name,memory.total --format=csv\n",
                "\n",
                "# Authenticate with HuggingFace\n",
                "from google.colab import userdata\n",
                "from huggingface_hub import login\n",
                "\n",
                "try:\n",
                "    hf_token = userdata.get('HF_TOKEN')\n",
                "    login(token=hf_token)\n",
                "    print(\"‚úÖ HuggingFace authentication successful!\")\n",
                "except Exception as e:\n",
                "    print(\"‚ùå HF_TOKEN not found!\")\n",
                "    print(\"   1. Go to https://huggingface.co/settings/tokens\")\n",
                "    print(\"   2. Create a token (Read access)\")\n",
                "    print(\"   3. Click the üîë key icon on the left sidebar\")\n",
                "    print(\"   4. Add secret: Name='HF_TOKEN', Value=your_token\")\n",
                "    print(\"   5. Restart runtime and try again\")"
            ],
            "metadata": {
                "id": "check_gpu"
            },
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Install Dependencies"
            ],
            "metadata": {
                "id": "step2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%capture\n",
                "!pip install transformers datasets peft accelerate bitsandbytes trl huggingface_hub"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Upload Training Data"
            ],
            "metadata": {
                "id": "step3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "import json\n",
                "\n",
                "print(\"üì§ Upload your functiongemma_training.jsonl file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "training_file = list(uploaded.keys())[0]\n",
                "with open(training_file, 'r') as f:\n",
                "    lines = f.readlines()\n",
                "print(f\"\\n‚úÖ Uploaded: {training_file} ({len(lines)} examples)\")"
            ],
            "metadata": {
                "id": "upload_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Load FunctionGemma 270M"
            ],
            "metadata": {
                "id": "step4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "\n",
                "MODEL_NAME = \"google/functiongemma-270m-it\"\n",
                "\n",
                "print(f\"üîÑ Loading {MODEL_NAME}...\")\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ FunctionGemma loaded! ({model.num_parameters():,} params)\")"
            ],
            "metadata": {
                "id": "load_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Apply LoRA"
            ],
            "metadata": {
                "id": "step5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total = sum(p.numel() for p in model.parameters())\n",
                "print(f\"‚úÖ LoRA applied! Trainable: {trainable:,}/{total:,} ({100*trainable/total:.2f}%)\")"
            ],
            "metadata": {
                "id": "apply_lora"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Prepare Dataset"
            ],
            "metadata": {
                "id": "step6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from datasets import Dataset\n",
                "\n",
                "data = []\n",
                "with open(training_file, 'r') as f:\n",
                "    for line in f:\n",
                "        entry = json.loads(line.strip())\n",
                "        data.append({\"text\": entry[\"prompt\"] + entry[\"completion\"]})\n",
                "\n",
                "dataset = Dataset.from_list(data)\n",
                "\n",
                "def tokenize(example):\n",
                "    return tokenizer(example[\"text\"], truncation=True, max_length=1024, padding=\"max_length\")\n",
                "\n",
                "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"text\"])\n",
                "print(f\"‚úÖ Dataset: {len(tokenized_dataset)} examples\")"
            ],
            "metadata": {
                "id": "prepare_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 7: Train! üöÄ"
            ],
            "metadata": {
                "id": "step7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./functiongemma-finetuned\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    warmup_ratio=0.03,\n",
                "    logging_steps=10,\n",
                "    save_steps=100,\n",
                "    fp16=True,\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    report_to=\"none\",\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
                ")\n",
                "\n",
                "print(\"üöÄ Starting training (~10-20 min)...\")\n",
                "trainer.train()\n",
                "print(\"‚úÖ Training complete!\")"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 8: Test Model"
            ],
            "metadata": {
                "id": "step8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "test_prompts = [\"Turn on the bedroom lights\", \"Set a timer for 5 minutes\", \"Hello\"]\n",
                "model.eval()\n",
                "\n",
                "# Get function declarations from training data\n",
                "with open(training_file, 'r') as f:\n",
                "    sample = json.loads(f.readline())\n",
                "\n",
                "print(\"üß™ Testing:\\n\")\n",
                "for prompt in test_prompts:\n",
                "    full_prompt = sample[\"prompt\"].rsplit(\"<start_of_turn>user\", 1)[0] + f\"<start_of_turn>user {prompt}<end_of_turn>\\n<start_of_turn>model\"\n",
                "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
                "    if \"<start_function_call>\" in response:\n",
                "        func = response.split(\"<start_function_call>\")[1].split(\"<end_function_call>\")[0]\n",
                "        print(f\"üìù {prompt}\\nü§ñ {func}\\n\")\n",
                "    else:\n",
                "        print(f\"üìù {prompt}\\nü§ñ {response[-150:]}\\n\")"
            ],
            "metadata": {
                "id": "test"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 9: Save & Download"
            ],
            "metadata": {
                "id": "step9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Save adapters\n",
                "model.save_pretrained(\"functiongemma-lora\")\n",
                "tokenizer.save_pretrained(\"functiongemma-lora\")\n",
                "!zip -r functiongemma-lora.zip functiongemma-lora/\n",
                "\n",
                "# Download\n",
                "files.download(\"functiongemma-lora.zip\")\n",
                "print(\"\\n‚úÖ Download started!\")"
            ],
            "metadata": {
                "id": "save"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 10: Export to GGUF for Ollama (Optional)"
            ],
            "metadata": {
                "id": "step10"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Merge adapters with base model\n",
                "from peft import PeftModel\n",
                "\n",
                "print(\"üîÑ Merging adapters...\")\n",
                "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n",
                "merged = PeftModel.from_pretrained(base, \"functiongemma-lora\").merge_and_unload()\n",
                "merged.save_pretrained(\"functiongemma-merged\")\n",
                "tokenizer.save_pretrained(\"functiongemma-merged\")\n",
                "print(\"‚úÖ Merged!\")"
            ],
            "metadata": {
                "id": "merge"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Convert to GGUF\n",
                "!pip install llama-cpp-python -q\n",
                "!git clone https://github.com/ggerganov/llama.cpp.git -q\n",
                "!pip install -r llama.cpp/requirements.txt -q\n",
                "!python llama.cpp/convert_hf_to_gguf.py functiongemma-merged --outfile functiongemma-finetuned.gguf --outtype q4_k_m\n",
                "\n",
                "files.download(\"functiongemma-finetuned.gguf\")\n",
                "print(\"‚úÖ GGUF downloaded!\")"
            ],
            "metadata": {
                "id": "gguf"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Use with Ollama\n",
                "\n",
                "```bash\n",
                "# Create Modelfile\n",
                "echo 'FROM ./functiongemma-finetuned.gguf\n",
                "PARAMETER temperature 0.1\n",
                "PARAMETER stop \"<end_of_turn>\"\n",
                "PARAMETER stop \"<end_function_call>\"' > Modelfile\n",
                "\n",
                "# Create and run\n",
                "ollama create functiongemma-custom -f Modelfile\n",
                "ollama run functiongemma-custom\n",
                "```"
            ],
            "metadata": {
                "id": "ollama"
            }
        }
    ]
}